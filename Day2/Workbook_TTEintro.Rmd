---
title: "Workbook for Introduction to TTE modeling"
author: "Introduction to TTE modeling"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
  # html_document:
  #   css: docs/src/styles/styles.css
  #   number_sections: true
  #   theme: united
  #   toc: true
  #   toc_float: true
params:
  include: TRUE
---


```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(comment='.',fig.align=TRUE,message=FALSE,warning=FALSE)
```


# Preliminaries for R examples

```{r, message=FALSE}
library(tidyverse)
library(stringr)
library(survival)
library(survminer)
library(texreg)
library(mgcv)

theme_set(theme_bw())
```


# Plotting hazard and survival functions over time for some common distributions

To better understand the relationship between the hazard, cumulative hazard, survival and density functions, let's plot them for a three commonly used parametric distributions in TTE modeling: exponential, Weibull, and log-normal.

## Exponential distribution

The hazard is constant as a funtion of time: $h(t) = \lambda$.

From first principles:
 * h(t) = lambda
 * H(t) = lambda * t
 * S(t) = exp(-lambda * t)
 * f(t) = h(t) * S(t) = lambda * exp(-lambda * t)
 
```{r}
plot_data <- crossing(lambda = c(0.5,1,2), times = seq(0,5,length=100)) %>% 
  mutate(hazard = lambda,
         cumulative_hazard = lambda * times,
         survival = exp(-cumulative_hazard),
         density = hazard * survival)

plot_data %>% 
  pivot_longer(cols=hazard:density, names_to = 'type', values_to='value') %>% 
  ggplot(aes(x=times, y=value, col=as.factor(lambda), group=lambda)) +
  geom_line() +
  facet_wrap(~type, scales = "free_y")
```


## Weibull

The Weibull distribution has two parameters: lambda and gamma

The log hazard is linear in the log of time: 
$$h(t) = \gamma \lambda t^{\gamma-1} \iff \log h(t) = \log\gamma + \log \lambda + (\gamma - 1) \log t$$


From first principles:
 * h(t) = lambda * gamma * t^(gamma-1)
 * H(t) = lambda * t^gamma
 * S(t) = exp(-lambda * t^gamma)
 * f(t) = h(t) * S(t) = lambda * gamma * t^(gamma-1) * exp(-lambda * t^gamma)

```{r}
plot_data <- crossing(lambda = c(0.5,2), gamma = c(0.5, 1, 3), times = seq(0,5,length=100)) %>% 
  mutate(hazard = lambda * gamma * times^(gamma-1),
         cumulative_hazard = lambda * times^gamma,
         survival = exp(-cumulative_hazard),
         density = hazard * survival
)

plot_data %>% 
  pivot_longer(cols=hazard:density, names_to = 'type', values_to='value') %>% 
  mutate(combination = paste(lambda,gamma,sep='-')) %>% 
  ggplot(aes(x=times, y=value, col=as.factor(gamma), group=combination)) +
  geom_line() +
  facet_grid(type~lambda, scales = "free_y")
```

## Log-normal

The hazard isn't easily expressed in closed form.  How can you derive it from first principles?  Edit the code below to make the plots.

Hint: the log normal pdf and survival function are obtained using the `dlnorm` and `plnorm` functions, respectively.

Plot the log-normal hazard function for combinations of mean (1, 2) and starndard deviation (0.3,1) modifying the code below.



```{r}
plot_data <- crossing(mean = c(1,2), sd = c(0.3, 1), times = seq(0,5,length=100)) %>% 
  mutate(survival = "insert survival function",
         density = "insert density function",
         cumulative_hazard = "insert cumulative hazard function",
         hazard = "insert hazard function"
)

plot_data %>% 
  pivot_longer(cols=survival:hazard, names_to = 'type', values_to='value') %>% 
  mutate(combination = paste(mean,sd,sep='-')) %>% 
  ggplot(aes(x=times, y=value, col=as.factor(sd), group=combination)) +
  geom_line() +
  facet_grid(type~mean, scales = "free_y")
```


***Bonus  exercise***

There  is no reason the hazard function needs to follow one of these functions, or any function associated with a 'named' probability distribution.

Make up your own hazard function and calculate  the cumulative hazard, survival and density functions.



# Estimating the survival function 

We'll use the `survfit` function to estimate the Survival function.  

Using our example data from the last class, let's estimate the overall S(t), i.e., not stratifting by any covariates.

```{r, "TRY toydata", purl=TRUE, results = "hide"}
load('../data/aedat.RDS')

aedat <-
  aedat %>% 
  mutate(AETOXGR = factor(aedat$AETOXGR, 0:3, labels=c("None","Mild","Moderate","Severe")),
         ae_any = AETOXGR != 'None') %>% 
  group_by(USUBJID) %>%
  # End of study for patients without a severe event
  mutate(TTE_SEVERE = case_when(
    STUDYID=="PROTA" ~ 2,
    STUDYID=="PROTB" ~ 6
  ),
  # Time of severe event for those that had one
  TTE_SEVERE = ifelse(AETOXGR=="Severe", TTE, TTE_SEVERE)
  )

# Both for EDA and for model-checking, it's generally helpful to have quartiles of exposure:
dat_use <-
  aedat %>% arrange(USUBJID, TTE_SEVERE) %>% slice(1) %>%
  group_by(PBO) %>%
  mutate(Quartile = ifelse(PBO == "PBO", "PBO",
                           paste0("Q", ntile(CAVGSS, n = 4))))

```


The key elements:

* `survfit`: the function used to obtain the K-M (or Flemming-Harrington)
* `Surv` funtion for defining the outcome

The `ggsurvplot

```{r}
km_est = survfit(Surv(TTE,ae_any)~1, data = dat_use)
ggsurvplot(km_est, risk.table = TRUE)
```
Often it's helpful to add the number of subjects at risk to the bottom of the plot (a risk table).


Plotting vs catrergorical covariatesa

How to handel continuous covariates?
  - Use optimal splitting ( Dan P)?


Your turn:
 * Plot K-M estimates for quartiles of exposure (`Quartile`)


# Semi-parametric models: Cox regression


# Cox model as a Poisson regression


# Piecewise exponential as an approximation


# smoothe hazard funtion




# Parametric survival models using survreg



## Parametric survival model using Stan/brms

The censoring variable should contain the values 'left', 'none', 'right', and 'interval' (or equivalently -1, 0, 1, and 2) to indicate that the corresponding observation is left censored, not censored, right censored, or interval censored. For interval censored data, a second variable (let's call it y2) has to be passed to cens. In this case, the formula has the structure y | cens(censored, y2) ~ predictors. 


```{r, "TRY firststan", purl=TRUE, results = "hide"}
mod01_stan <- brm(formula = TTE | cens(1-ae_any) ~ Quartile,  
                  data=dat_use, 
                  family = weibull(),
                  warmup = 500, 
                  iter = 2000, 
                  chains = 4, 
                  inits = "0", 
                  cores = 4,
                  seed = 123)
```

Assess convergence (traceplots, rhat)

```{r}
rhat(mod01_stan)
```

```{r}
plot
```


Look at posterior distributions

Calculate model fit summary

```{r}
loo(mod01_stan)
```

Posterior predictive distributions

```{r}
# Generate posterior predictive samples (100 posterior samples)
post_preds <- dat_use %>% 
  add_predicted_draws(mod01_stan, n=100) %>%
  mutate(
    # TODO: Use a more general plug-in censoring function that is a function of data and .prediction
    # Censor using same process as in the source data
    .simdv = if_else(.prediction < STUDYDUR, 1, 0),
    .simtte = if_else(.prediction >= STUDYDUR, STUDYDUR,.prediction)) %>% 
  ungroup()

# Summarise posterior predictions at a grid of times
times = seq(0,6,length=100)

post_preds <- post_preds %>% 
  nest(sim_data = -.draw) %>% 
  bind_rows(dat_use %>% mutate(.draw=0, .simtte=TTE, .simdv=ae_any) %>% nest(sim_data = -.draw)) %>% 
  mutate(
    # Kaplan-Meier fits to each simulated dataset
    km = map(sim_data, ~survfit(Surv(.simtte, .simdv)~Quartile, data = .x)),
    # Extract predictions at a grid of times
    preds = map2(km, sim_data, function(.x,.y){
      .simfit = summary(.x, times=times)
      tibble(strata = .simfit$strata,
             surv = .simfit$surv,
             time = .simfit$time)
    })
  )

# Obtain median and quantiles for simulated data
post_pred_summary <- post_preds %>% 
  filter(.draw > 0) %>% 
  select(.draw, preds) %>% 
  unnest(cols=preds) %>% 
  group_by(strata, time) %>% 
  summarise(lpl = quantile(surv, probs = 0.05),
            upl = quantile(surv, probs = 0.95),
            surv = median(surv),
            n=n()) %>% 
  mutate(type = 'Simulated')


obs_summary <- post_preds %>% 
  filter(.draw == 0) %>% 
  select(preds) %>% 
  unnest(cols=preds) %>% 
  mutate(type='Observed')
  
  
  
bind_rows(obs_summary,post_pred_summary) %>% 
  ggplot(aes(x = time)) +
  geom_step(aes(y = surv, color=type)) +
  geom_ribbon(aes(ymin=lpl, ymax=upl, fill=type), alpha=0.25) +
  facet_wrap(~strata) +
  ylim(0,1)
  
```


Fit alternative model

Compare via loo and vpc stratified my exposure.  Which model do you prefer and why?

