
@ARTICLE{Jahn-Eimermacher2015-rz,
  title    = "Simulating recurrent event data with hazard functions defined on
              a total time scale",
  author   = "Jahn-Eimermacher, Antje and Ingel, Katharina and Ozga,
              Ann-Kathrin and Preussler, Stella and Binder, Harald",
  abstract = "BACKGROUND: In medical studies with recurrent event data a total
              time scale perspective is often needed to adequately reflect
              disease mechanisms. This means that the hazard process is defined
              on the time since some starting point, e.g. the beginning of some
              disease, in contrast to a gap time scale where the hazard process
              restarts after each event. While techniques such as the
              Andersen-Gill model have been developed for analyzing data from a
              total time perspective, techniques for the simulation of such
              data, e.g. for sample size planning, have not been investigated
              so far. METHODS: We have derived a simulation algorithm covering
              the Andersen-Gill model that can be used for sample size planning
              in clinical trials as well as the investigation of modeling
              techniques. Specifically, we allow for fixed and/or random
              covariates and an arbitrary hazard function defined on a total
              time scale. Furthermore we take into account that individuals may
              be temporarily insusceptible to a recurrent incidence of the
              event. The methods are based on conditional distributions of the
              inter-event times conditional on the total time of the preceeding
              event or study start. Closed form solutions are provided for
              common distributions. The derived methods have been implemented
              in a readily accessible R script. RESULTS: The proposed
              techniques are illustrated by planning the sample size for a
              clinical trial with complex recurrent event data. The required
              sample size is shown to be affected not only by censoring and
              intra-patient correlation, but also by the presence of risk-free
              intervals. This demonstrates the need for a simulation algorithm
              that particularly allows for complex study designs where no
              analytical sample size formulas might exist. CONCLUSIONS: The
              derived simulation algorithm is seen to be useful for the
              simulation of recurrent event data that follow an Andersen-Gill
              model. Next to the use of a total time scale, it allows for
              intra-patient correlation and risk-free intervals as are often
              observed in clinical trial data. Its application therefore allows
              the simulation of data that closely resemble real settings and
              thus can improve the use of simulation studies for designing and
              analysing studies.",
  journal  = "BMC Med. Res. Methodol.",
  volume   =  15,
  pages    = "16",
  month    =  mar,
  year     =  2015,
  language = "en"
}

@ARTICLE{Balan2020-wv,
  title    = "A tutorial on frailty models",
  author   = "Balan, Theodor A and Putter, Hein",
  abstract = "The hazard function plays a central role in survival analysis. In
              a homogeneous population, the distribution of the time to event,
              described by the hazard, is the same for each individual.
              Heterogeneity in the distributions can be accounted for by
              including covariates in a model for the hazard, for instance a
              proportional hazards model. In this model, individuals with the
              same value of the covariates will have the same distribution. It
              is natural to think that not all covariates that are thought to
              influence the distribution of the survival outcome are included
              in the model. This implies that there is unobserved
              heterogeneity; individuals with the same value of the covariates
              may have different distributions. One way of accounting for this
              unobserved heterogeneity is to include random effects in the
              model. In the context of hazard models for time to event
              outcomes, such random effects are called frailties, and the
              resulting models are called frailty models. In this tutorial, we
              study frailty models for survival outcomes. We illustrate how
              frailties induce selection of healthier individuals among
              survivors, and show how shared frailties can be used to model
              positively dependent survival outcomes in clustered data. The
              Laplace transform of the frailty distribution plays a central
              role in relating the hazards, conditional on the frailty, to
              hazards and survival functions observed in a population.
              Available software, mainly in R, will be discussed, and the use
              of frailty models is illustrated in two different applications,
              one on center effects and the other on recurrent events.",
  journal  = "Stat. Methods Med. Res.",
  volume   =  29,
  number   =  11,
  pages    = "3424--3454",
  month    =  nov,
  year     =  2020,
  keywords = "Correlated failure times; frailty models; random effects models;
              survival analysis; unobserved heterogeneity",
  language = "en"
}

@ARTICLE{Abrantes2020-yd,
  title    = "Relationship between factor {VIII} activity, bleeds and
              individual characteristics in severe hemophilia A patients",
  author   = "Abrantes, Jo{\~a}o A and Solms, Alexander and Garmann, Dirk and
              Nielsen, Elisabet I and J{\"o}nsson, Siv and Karlsson, Mats O",
  abstract = "Pharmacokinetic-based prophylaxis of replacement factor VIII
              (FVIII) products has been encouraged in recent years, but the
              relationship between exposure (factor VIII activity) and response
              (bleeding frequency) remains unclear. The aim of this study was
              to characterize the relationship between FVIII dose, plasma FVIII
              activity, and bleeding patterns and individual characteristics in
              severe hemophilia A patients. Pooled pharmacokinetic and bleeding
              data during prophylactic treatment with BAY 81-8973 (octocog
              alfa) were obtained from the three LEOPOLD trials. The population
              pharmacokinetics of FVIII activity and longitudinal bleeding
              frequency, as well as bleeding severity, were described using
              non-linear mixed effects modeling in NONMEM. In total, 183
              patients [median age 22 years (range, 1-61); weight 60 kg
              (11-124)] contributed with 1,535 plasma FVIII activity
              observations, 633 bleeds and 11 patient/study characteristics
              [median observation period 12 months (3.1-13.1)]. A parametric
              repeated time-to-categorical bleed model, guided by plasma FVIII
              activity from a 2-compartment population pharmacokinetic model,
              described the time to the occurrence of bleeds and their
              severity. Bleeding probability decreased with time of study, and
              a bleed was not found to affect the time of the next bleed.
              Several covariate effects were identified, including the bleeding
              history in the 12-month pre-study period increasing the bleeding
              hazard. However, unexplained inter-patient variability in the
              phenotypic bleeding pattern remained large (111\%CV). Further
              studies to translate the model into a tool for dose
              individualization that considers the individual bleeding risk are
              required. Research was based on a post-hoc analysis of the
              LEOPOLD studies registered at clinicaltrials.gov identifiers:
              01029340, 01233258 and 01311648.",
  journal  = "Haematologica",
  volume   =  105,
  number   =  5,
  pages    = "1443--1453",
  month    =  may,
  year     =  2020,
  language = "en"
}

@ARTICLE{Hoffman2014-xe,
  title     = "The {No-U-turn} sampler: adaptively setting path lengths in
               Hamiltonian Monte Carlo",
  author    = "Hoffman, Matthew D and Gelman, Andrew",
  abstract  = "Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo
               (MCMC) algorithm that avoids the random walk behavior and
               sensitivity to correlated parameters that plague many MCMC
               methods by taking a series of steps informed by first-order
               gradient information. These features allow it to converge to
               high-dimensional target distributions much more quickly than
               simpler methods such as random walk Metropolis or Gibbs
               sampling. However, HMC's performance is highly sensitive to two
               user-specified parameters: a step size $\epsilon$ and a desired
               number of steps L. In particular, if L is too small then the
               algorithm exhibits undesirable random walk behavior, while if L
               is too large the algorithm wastes computation. We introduce the
               No-U-Turn Sampler (NUTS), an extension to HMC that eliminates
               the need to set a number of steps L. NUTS uses a recursive
               algorithm to build a set of likely candidate points that spans a
               wide swath of the target distribution, stopping automatically
               when it starts to double back and retrace its steps.
               Empirically, NUTS performs at least as efficiently as (and
               sometimes more effciently than) a well tuned standard HMC
               method, without requiring user intervention or costly tuning
               runs. We also derive a method for adapting the step size
               parameter $\epsilon$ on the fly based on primal-dual averaging.
               NUTS can thus be used with no hand-tuning at all, making it
               suitable for applications such as BUGS-style automatic inference
               engines that require efficient ``turnkey'' samplers.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  15,
  number    =  1,
  pages     = "1593--1623",
  month     =  jan,
  year      =  2014,
  keywords  = "Markov chain Monte Carlo, Hamiltonian Monte Carlo, dual
               averaging, adaptive Monte Carlo, Bayesian inference"
}

@ARTICLE{Betancourt2017-tb,
  title         = "A Conceptual Introduction to Hamiltonian Monte Carlo",
  author        = "Betancourt, Michael",
  abstract      = "Hamiltonian Monte Carlo has proven a remarkable empirical
                   success, but only recently have we begun to develop a
                   rigorous understanding of why it performs so well on
                   difficult problems and how it is best applied in practice.
                   Unfortunately, that understanding is confined within the
                   mathematics of differential geometry which has limited its
                   dissemination, especially to the applied communities for
                   which it is particularly important. In this review I provide
                   a comprehensive conceptual account of these theoretical
                   foundations, focusing on developing a principled intuition
                   behind the method and its optimal implementations rather of
                   any exhaustive rigor. Whether a practitioner or a
                   statistician, the dedicated reader will acquire a solid
                   grasp of how Hamiltonian Monte Carlo works, when it
                   succeeds, and, perhaps most importantly, when it fails.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1701.02434"
}



@ARTICLE{Zecchin2016-rw,
  title    = "Models for change in tumour size, appearance of new lesions and
              survival probability in patients with advanced epithelial ovarian
              cancer",
  author   = "Zecchin, Chiara and Gueorguieva, Ivelina and Enas, Nathan H and
              Friberg, Lena E",
  abstract = "AIMS: The aims of this study were (i) to develop a modelling
              framework linking change in tumour size during treatment to
              survival probability in metastatic ovarian cancer; and (ii) to
              model the appearance of new lesions and investigate their
              relationship with survival and disease characteristics. METHODS:
              Data from a randomized Phase III clinical trial comparing
              carboplatin monotherapy to gemcitabine plus carboplatin
              combotherapy in 336 patients with metastatic ovarian cancer were
              used. A population model describing change in tumour size based
              on drug treatment information was established and its
              relationship with time to appearance of new lesions and survival
              were investigated with time to event models. RESULTS: The tumour
              size profiles were well characterized as evaluated by visual
              predictive checks. Metastasis in the liver at enrolment and
              change in tumour size up to week 12 were predictors of time to
              appearance of new lesions. Survival was predicted based on the
              patient tumour size and ECOG performance status at enrolment and
              on appearance of new lesions during treatment and change in
              tumour size up to week 12. Tumour size and survival data from a
              separate study were adequately predicted. CONCLUSIONS: The
              proposed models simulate tumour dynamics following treatment and
              provide a link to the probability of developing new lesions as
              well as to survival. The models have potential to be used for
              optimizing the design of late phase clinical trials in metastatic
              ovarian cancer based on early phase clinical study results and
              simulation.",
  journal  = "Br. J. Clin. Pharmacol.",
  volume   =  82,
  number   =  3,
  pages    = "717--727",
  month    =  sep,
  year     =  2016,
  keywords = "Phase III; carboplatin; gemcitabine; metastasis",
  language = "en"
}

@INCOLLECTION{noauthor_2011-gh,
  title     = "Appendix 2: An introduction to the counting process approach to
               survival analysis",
  booktitle = "Applied Survival Analysis",
  publisher = "John Wiley \& Sons, Inc.",
  pages     = "359--363",
  month     =  oct,
  year      =  2011,
  address   = "Hoboken, NJ, USA"
}

@ARTICLE{Whitehead1980-fl,
  title     = "Fitting Cox's Regression Model to Survival Data using {GLIM}",
  author    = "Whitehead, John",
  abstract  = "[The proportional hazard regression model is reviewed, and its
               analysis using GLIM is described. Methods of estimating the
               underlying survivor functions are discussed. The Poisson model
               which allows the use of GLIM is introduced and interpreted. Two
               different treatments of tied observations are mentioned, and
               their properties are compared in the context of a specific
               example.]",
  journal   = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  publisher = "[Wiley, Royal Statistical Society]",
  volume    =  29,
  number    =  3,
  pages     = "268--275",
  year      =  1980
}


@ARTICLE{French2012-qv,
  title     = "Using Historical Data With Bayesian Methods in Early Clinical
               Trial Monitoring",
  author    = "French, Jonathan L and Thomas, Neal and Wang, Cunshan",
  abstract  = "This article presents methods that were used to monitor two
               recent clinical trials for the early development of a novel
               therapeutic compound. We present Bayesian methods that were used
               to incorporate historical control data for monitoring early in
               the trials when there were very limited trial data. However,
               decisions to continue the study or terminate the experimental
               treatment had to be made to protect the patients in the trial.
               In the first trial, a pause in accrual was planned to allow
               assessment before a larger cohort could be recruited using more
               clinical centers. The primary endpoint for the continuation
               decision was binary, with accrual suspended until all patients
               in the first phase reached the endpoint time. The Bayesian
               method replaced decision criteria that regarded the historical
               data as a known standard, with the concurrent control data used
               to informally assist interpretation by the study clinicians.
               Monitoring of the subsequent study was planned without pauses in
               accrual. Survival methods used the partial information available
               on the eventual longer-term binary outcome. The Bayesian methods
               were approximately calibrated to avoid terminating the
               experimental compound incorrectly, and they were compared with
               some common, similarly calibrated non-Bayesian methods. The
               Bayesian methods were more likely to stop the experimental
               treatment correctly under conditions likely to arise in the
               trial.",
  journal   = "Stat. Biopharm. Res.",
  publisher = "Taylor \& Francis",
  volume    =  4,
  number    =  4,
  pages     = "384--394",
  month     =  oct,
  year      =  2012
}


@ARTICLE{Royston2002-vc,
  title    = "Flexible parametric proportional-hazards and proportional-odds
              models for censored survival data, with application to prognostic
              modelling and estimation of treatment effects",
  author   = "Royston, Patrick and Parmar, Mahesh K B",
  abstract = "Modelling of censored survival data is almost always done by Cox
              proportional-hazards regression. However, use of parametric
              models for such data may have some advantages. For example,
              non-proportional hazards, a potential difficulty with Cox models,
              may sometimes be handled in a simple way, and visualization of
              the hazard function is much easier. Extensions of the Weibull and
              log-logistic models are proposed in which natural cubic splines
              are used to smooth the baseline log cumulative hazard and log
              cumulative odds of failure functions. Further extensions to allow
              non-proportional effects of some or all of the covariates are
              introduced. A hypothesis test of the appropriateness of the scale
              chosen for covariate effects (such as of treatment) is proposed.
              The new models are applied to two data sets in cancer. The
              results throw interesting light on the behaviour of both the
              hazard function and the hazard ratio over time. The tools
              described here may be a step towards providing greater insight
              into the natural history of the disease and into possible
              underlying causes of clinical events. We illustrate these aspects
              by using the two examples in cancer.",
  journal  = "Stat. Med.",
  volume   =  21,
  number   =  15,
  pages    = "2175--2197",
  month    =  aug,
  year     =  2002,
  language = "en"
}


@ARTICLE{Holford2013-mh,
  title    = "A time to event tutorial for pharmacometricians",
  author   = "Holford, Nick",
  journal  = "CPT Pharmacometrics Syst Pharmacol",
  volume   =  2,
  pages    = "e43",
  month    =  may,
  year     =  2013,
  language = "en"
}


@ARTICLE{Cox1972-ul,
  title     = "Regression models and life-tables",
  author    = "Cox, D R",
  abstract  = "Summary The analysis of censored failure times is considered. It
               is assumed that on each individual are available values of one
               or more explanatory variables. The hazard function (age-specific
               failure rate) is taken to be a function of the explanatory
               variables and unknown regression coefficients multiplied by an
               arbitrary and unknown function of time. A conditional likelihood
               is obtained, leading to inferences about the unknown regression
               coefficients. Some generalizations are outlined.",
  journal   = "J. R. Stat. Soc.",
  publisher = "Wiley",
  volume    =  34,
  number    =  2,
  pages     = "187--202",
  month     =  jan,
  year      =  1972,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}


@ARTICLE{Harrell1982-ux,
  title    = "Evaluating the yield of medical tests",
  author   = "Harrell, Jr, F E and Califf, R M and Pryor, D B and Lee, K L and
              Rosati, R A",
  abstract = "A method is presented for evaluating the amount of information a
              medical test provides about individual patients. Emphasis is
              placed on the role of a test in the evaluation of patients with a
              chronic disease. In this context, the yield of a test is best
              interpreted by analyzing the prognostic information it furnishes.
              Information from the history, physical examination, and routine
              procedures should be used in assessing the yield of a new test.
              As an example, the method is applied to the use of the treadmill
              exercise test in evaluating the prognosis of patients with
              suspected coronary artery disease. The treadmill test is shown to
              provide surprisingly little prognostic information beyond that
              obtained from basic clinical measurements.",
  journal  = "JAMA",
  volume   =  247,
  number   =  18,
  pages    = "2543--2546",
  month    =  may,
  year     =  1982,
  language = "en"
}


@ARTICLE{Wei1992-lp,
  title    = "The accelerated failure time model: a useful alternative to the
              Cox regression model in survival analysis",
  author   = "Wei, L J",
  abstract = "For the past two decades the Cox proportional hazards model has
              been used extensively to examine the covariate effects on the
              hazard function for the failure time variable. On the other hand,
              the accelerated failure time model, which simply regresses the
              logarithm of the survival time over the covariates, has seldom
              been utilized in the analysis of censored survival data. In this
              article, we review some newly developed linear regression methods
              for analysing failure time observations. These procedures have
              sound theoretical justification and can be implemented with an
              efficient numerical method. The accelerated failure time model
              has an intuitive physical interpretation and would be a useful
              alternative to the Cox model in survival analysis.",
  journal  = "Stat. Med.",
  volume   =  11,
  number   = "14-15",
  pages    = "1871--1879",
  month    =  oct,
  year     =  1992,
  language = "en"
}


@ARTICLE{Bradburn2003-fg,
  title    = "Survival analysis part {II}: multivariate data analysis--an
              introduction to concepts and methods",
  author   = "Bradburn, M J and Clark, T G and Love, S B and Altman, D G",
  journal  = "Br. J. Cancer",
  volume   =  89,
  number   =  3,
  pages    = "431--436",
  month    =  aug,
  year     =  2003,
  language = "en"
}



