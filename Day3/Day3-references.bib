
@ARTICLE{Hoffman2014-xe,
  title     = "The {No-U-turn} sampler: adaptively setting path lengths in
               Hamiltonian Monte Carlo",
  author    = "Hoffman, Matthew D and Gelman, Andrew",
  abstract  = "Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo
               (MCMC) algorithm that avoids the random walk behavior and
               sensitivity to correlated parameters that plague many MCMC
               methods by taking a series of steps informed by first-order
               gradient information. These features allow it to converge to
               high-dimensional target distributions much more quickly than
               simpler methods such as random walk Metropolis or Gibbs
               sampling. However, HMC's performance is highly sensitive to two
               user-specified parameters: a step size $\epsilon$ and a desired
               number of steps L. In particular, if L is too small then the
               algorithm exhibits undesirable random walk behavior, while if L
               is too large the algorithm wastes computation. We introduce the
               No-U-Turn Sampler (NUTS), an extension to HMC that eliminates
               the need to set a number of steps L. NUTS uses a recursive
               algorithm to build a set of likely candidate points that spans a
               wide swath of the target distribution, stopping automatically
               when it starts to double back and retrace its steps.
               Empirically, NUTS performs at least as efficiently as (and
               sometimes more effciently than) a well tuned standard HMC
               method, without requiring user intervention or costly tuning
               runs. We also derive a method for adapting the step size
               parameter $\epsilon$ on the fly based on primal-dual averaging.
               NUTS can thus be used with no hand-tuning at all, making it
               suitable for applications such as BUGS-style automatic inference
               engines that require efficient ``turnkey'' samplers.",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  15,
  number    =  1,
  pages     = "1593--1623",
  month     =  jan,
  year      =  2014,
  keywords  = "Markov chain Monte Carlo, Hamiltonian Monte Carlo, dual
               averaging, adaptive Monte Carlo, Bayesian inference"
}

@ARTICLE{Betancourt2017-tb,
  title         = "A Conceptual Introduction to Hamiltonian Monte Carlo",
  author        = "Betancourt, Michael",
  abstract      = "Hamiltonian Monte Carlo has proven a remarkable empirical
                   success, but only recently have we begun to develop a
                   rigorous understanding of why it performs so well on
                   difficult problems and how it is best applied in practice.
                   Unfortunately, that understanding is confined within the
                   mathematics of differential geometry which has limited its
                   dissemination, especially to the applied communities for
                   which it is particularly important. In this review I provide
                   a comprehensive conceptual account of these theoretical
                   foundations, focusing on developing a principled intuition
                   behind the method and its optimal implementations rather of
                   any exhaustive rigor. Whether a practitioner or a
                   statistician, the dedicated reader will acquire a solid
                   grasp of how Hamiltonian Monte Carlo works, when it
                   succeeds, and, perhaps most importantly, when it fails.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1701.02434"
}



@ARTICLE{Zecchin2016-rw,
  title    = "Models for change in tumour size, appearance of new lesions and
              survival probability in patients with advanced epithelial ovarian
              cancer",
  author   = "Zecchin, Chiara and Gueorguieva, Ivelina and Enas, Nathan H and
              Friberg, Lena E",
  abstract = "AIMS: The aims of this study were (i) to develop a modelling
              framework linking change in tumour size during treatment to
              survival probability in metastatic ovarian cancer; and (ii) to
              model the appearance of new lesions and investigate their
              relationship with survival and disease characteristics. METHODS:
              Data from a randomized Phase III clinical trial comparing
              carboplatin monotherapy to gemcitabine plus carboplatin
              combotherapy in 336 patients with metastatic ovarian cancer were
              used. A population model describing change in tumour size based
              on drug treatment information was established and its
              relationship with time to appearance of new lesions and survival
              were investigated with time to event models. RESULTS: The tumour
              size profiles were well characterized as evaluated by visual
              predictive checks. Metastasis in the liver at enrolment and
              change in tumour size up to week 12 were predictors of time to
              appearance of new lesions. Survival was predicted based on the
              patient tumour size and ECOG performance status at enrolment and
              on appearance of new lesions during treatment and change in
              tumour size up to week 12. Tumour size and survival data from a
              separate study were adequately predicted. CONCLUSIONS: The
              proposed models simulate tumour dynamics following treatment and
              provide a link to the probability of developing new lesions as
              well as to survival. The models have potential to be used for
              optimizing the design of late phase clinical trials in metastatic
              ovarian cancer based on early phase clinical study results and
              simulation.",
  journal  = "Br. J. Clin. Pharmacol.",
  volume   =  82,
  number   =  3,
  pages    = "717--727",
  month    =  sep,
  year     =  2016,
  keywords = "Phase III; carboplatin; gemcitabine; metastasis",
  language = "en"
}

@INCOLLECTION{noauthor_2011-gh,
  title     = "Appendix 2: An introduction to the counting process approach to
               survival analysis",
  booktitle = "Applied Survival Analysis",
  publisher = "John Wiley \& Sons, Inc.",
  pages     = "359--363",
  month     =  oct,
  year      =  2011,
  address   = "Hoboken, NJ, USA"
}

@ARTICLE{Whitehead1980-fl,
  title     = "Fitting Cox's Regression Model to Survival Data using {GLIM}",
  author    = "Whitehead, John",
  abstract  = "[The proportional hazard regression model is reviewed, and its
               analysis using GLIM is described. Methods of estimating the
               underlying survivor functions are discussed. The Poisson model
               which allows the use of GLIM is introduced and interpreted. Two
               different treatments of tied observations are mentioned, and
               their properties are compared in the context of a specific
               example.]",
  journal   = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  publisher = "[Wiley, Royal Statistical Society]",
  volume    =  29,
  number    =  3,
  pages     = "268--275",
  year      =  1980
}


@ARTICLE{French2012-qv,
  title     = "Using Historical Data With Bayesian Methods in Early Clinical
               Trial Monitoring",
  author    = "French, Jonathan L and Thomas, Neal and Wang, Cunshan",
  abstract  = "This article presents methods that were used to monitor two
               recent clinical trials for the early development of a novel
               therapeutic compound. We present Bayesian methods that were used
               to incorporate historical control data for monitoring early in
               the trials when there were very limited trial data. However,
               decisions to continue the study or terminate the experimental
               treatment had to be made to protect the patients in the trial.
               In the first trial, a pause in accrual was planned to allow
               assessment before a larger cohort could be recruited using more
               clinical centers. The primary endpoint for the continuation
               decision was binary, with accrual suspended until all patients
               in the first phase reached the endpoint time. The Bayesian
               method replaced decision criteria that regarded the historical
               data as a known standard, with the concurrent control data used
               to informally assist interpretation by the study clinicians.
               Monitoring of the subsequent study was planned without pauses in
               accrual. Survival methods used the partial information available
               on the eventual longer-term binary outcome. The Bayesian methods
               were approximately calibrated to avoid terminating the
               experimental compound incorrectly, and they were compared with
               some common, similarly calibrated non-Bayesian methods. The
               Bayesian methods were more likely to stop the experimental
               treatment correctly under conditions likely to arise in the
               trial.",
  journal   = "Stat. Biopharm. Res.",
  publisher = "Taylor \& Francis",
  volume    =  4,
  number    =  4,
  pages     = "384--394",
  month     =  oct,
  year      =  2012
}


@ARTICLE{Royston2002-vc,
  title    = "Flexible parametric proportional-hazards and proportional-odds
              models for censored survival data, with application to prognostic
              modelling and estimation of treatment effects",
  author   = "Royston, Patrick and Parmar, Mahesh K B",
  abstract = "Modelling of censored survival data is almost always done by Cox
              proportional-hazards regression. However, use of parametric
              models for such data may have some advantages. For example,
              non-proportional hazards, a potential difficulty with Cox models,
              may sometimes be handled in a simple way, and visualization of
              the hazard function is much easier. Extensions of the Weibull and
              log-logistic models are proposed in which natural cubic splines
              are used to smooth the baseline log cumulative hazard and log
              cumulative odds of failure functions. Further extensions to allow
              non-proportional effects of some or all of the covariates are
              introduced. A hypothesis test of the appropriateness of the scale
              chosen for covariate effects (such as of treatment) is proposed.
              The new models are applied to two data sets in cancer. The
              results throw interesting light on the behaviour of both the
              hazard function and the hazard ratio over time. The tools
              described here may be a step towards providing greater insight
              into the natural history of the disease and into possible
              underlying causes of clinical events. We illustrate these aspects
              by using the two examples in cancer.",
  journal  = "Stat. Med.",
  volume   =  21,
  number   =  15,
  pages    = "2175--2197",
  month    =  aug,
  year     =  2002,
  language = "en"
}


@ARTICLE{Holford2013-mh,
  title    = "A time to event tutorial for pharmacometricians",
  author   = "Holford, Nick",
  journal  = "CPT Pharmacometrics Syst Pharmacol",
  volume   =  2,
  pages    = "e43",
  month    =  may,
  year     =  2013,
  language = "en"
}


@ARTICLE{Cox1972-ul,
  title     = "Regression models and life-tables",
  author    = "Cox, D R",
  abstract  = "Summary The analysis of censored failure times is considered. It
               is assumed that on each individual are available values of one
               or more explanatory variables. The hazard function (age-specific
               failure rate) is taken to be a function of the explanatory
               variables and unknown regression coefficients multiplied by an
               arbitrary and unknown function of time. A conditional likelihood
               is obtained, leading to inferences about the unknown regression
               coefficients. Some generalizations are outlined.",
  journal   = "J. R. Stat. Soc.",
  publisher = "Wiley",
  volume    =  34,
  number    =  2,
  pages     = "187--202",
  month     =  jan,
  year      =  1972,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}


@ARTICLE{Harrell1982-ux,
  title    = "Evaluating the yield of medical tests",
  author   = "Harrell, Jr, F E and Califf, R M and Pryor, D B and Lee, K L and
              Rosati, R A",
  abstract = "A method is presented for evaluating the amount of information a
              medical test provides about individual patients. Emphasis is
              placed on the role of a test in the evaluation of patients with a
              chronic disease. In this context, the yield of a test is best
              interpreted by analyzing the prognostic information it furnishes.
              Information from the history, physical examination, and routine
              procedures should be used in assessing the yield of a new test.
              As an example, the method is applied to the use of the treadmill
              exercise test in evaluating the prognosis of patients with
              suspected coronary artery disease. The treadmill test is shown to
              provide surprisingly little prognostic information beyond that
              obtained from basic clinical measurements.",
  journal  = "JAMA",
  volume   =  247,
  number   =  18,
  pages    = "2543--2546",
  month    =  may,
  year     =  1982,
  language = "en"
}


@ARTICLE{Wei1992-lp,
  title    = "The accelerated failure time model: a useful alternative to the
              Cox regression model in survival analysis",
  author   = "Wei, L J",
  abstract = "For the past two decades the Cox proportional hazards model has
              been used extensively to examine the covariate effects on the
              hazard function for the failure time variable. On the other hand,
              the accelerated failure time model, which simply regresses the
              logarithm of the survival time over the covariates, has seldom
              been utilized in the analysis of censored survival data. In this
              article, we review some newly developed linear regression methods
              for analysing failure time observations. These procedures have
              sound theoretical justification and can be implemented with an
              efficient numerical method. The accelerated failure time model
              has an intuitive physical interpretation and would be a useful
              alternative to the Cox model in survival analysis.",
  journal  = "Stat. Med.",
  volume   =  11,
  number   = "14-15",
  pages    = "1871--1879",
  month    =  oct,
  year     =  1992,
  language = "en"
}


@ARTICLE{Bradburn2003-fg,
  title    = "Survival analysis part {II}: multivariate data analysis--an
              introduction to concepts and methods",
  author   = "Bradburn, M J and Clark, T G and Love, S B and Altman, D G",
  journal  = "Br. J. Cancer",
  volume   =  89,
  number   =  3,
  pages    = "431--436",
  month    =  aug,
  year     =  2003,
  language = "en"
}



