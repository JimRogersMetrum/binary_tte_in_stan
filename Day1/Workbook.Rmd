---
title: "Workbook"
author: "Introduction to GLMs for exposure-response with binary endpoints"
date: "`r Sys.Date()`"
output:
  html_document:
    css: docs/src/styles/styles.css
    number_sections: true
    theme: united
    toc: true
    toc_float: true
params:
  include: TRUE
---


```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(comment='.',fig.align=TRUE,message=FALSE,warning=FALSE)
```


# Preliminaries for R examples

```{r, message=FALSE}
library(tidyverse)
library(stringr)
library(haven)
library(GGally)
library(binom)
library(texreg)
library(mgcv)
library(DHARMa)

expit <- function(x) 1 / (1+exp(-x))
```

# Exploring the binomial density

## rbinom

```{r, "TRY rbinom", purl=TRUE, results = "hide"}
set.seed(314159)
y <- rbinom(n=20, size=1, prob=0.5)
table(y)
```

Exercise:
* Generate 100 independent Bernoulli observations with success probability = 0.2.
* Do the same thing again, both with and without re-setting the "seed"

```{r}

```


## dbinom

```{r, "dbinom", purl=TRUE, results = "hide"}
set.seed(1)
pp <- 0.7 # probability that event happens
y <- 1 # indicates event occured.
nn <- 1 # number of "attempts"
pp^y*(1-pp)^(nn-y)
dbinom(x=y, size=nn, prob=pp) # same thing
y <- c(0, 0, 0, 1, 0, 1)
# Likelihood contributions, evaluated at p = 0.1
dbinom(x=y, size=1, prob=0.1)
```


## likelihood computation

```{r, "TRY dbinom", purl=TRUE, results = "hide"}
set.seed(1)
# Joint likelihood, evaluated at p = 0.1
prod(dbinom(x=y, size=1, prob=0.1))
```

Exercise do one of the following:

* Use trial and error to find the probability that maximizes the joint likelihood.
* Use some more clever trick to accomplish the same goal.

```{r}

```


## likelihood maximization

Advanced exercise: look up `?optim` and use that to maximize the joint likelihood

```{r}

```



# Workbook: Visualizing binary data

```{r, "TRY toydata", purl=TRUE, results = "hide"}
load('../data/aedat.RDS')
str(aedat)
```

* This data set is comprised of a two week study (protocol A) and a six week study (protocol B)
* The data set includes patients of type `PT2`, a patient type that (let's say) has not been studied at higher dose levels.
* Suppose further that a dose level under consideration for a phase 2 study in `PT2` would have typical value exposures near 2.5 ug/mL.
* The objective of *this* anlaysis is to determine whether the AE rate will be "sufficiently low" in `PT2` at that exposure.


## EDA: data key

```{r, "TRY datakey", purl=TRUE, results = "hide"}
aedat$AETOXGR <- factor(aedat$AETOXGR, 0:3, labels=c("None","Mild","Moderate","Severe"))

(aedat %>% group_by(USUBJID) %>% n_groups()) == nrow(aedat)

aedat %>% ungroup() %>% count(AETOXGR,AE01)

aedat %>% arrange(USUBJID, TTE) %>% group_by(USUBJID) %>% filter(n()>1)

# Create a variable for time-to-serious AE's, matching "AE01"
aedat <-
  aedat %>% group_by(USUBJID) %>%
  # End of study for patients without a severe event
  mutate(TTE_SEVERE = case_when(
    STUDYID=="PROTA" ~ 2,
    STUDYID=="PROTB" ~ 6
  ),
  # Time of severe event for those that had one
  TTE_SEVERE = ifelse(AETOXGR=="Severe", TTE, TTE_SEVERE)
  )
```


Exercise: evaluate the preceding code and based on that answer the following:

* If we fit a model for `AE01` (severe events), is it reasonable to treat each row as independent? If not, what can be done to address this issue?
* If we fit a model for `AE01`, what are the implications of combining data from studies of two different durations?
* How can we assess whether the differing durations are likely to be problematic?



## Derived variables

Both for EDA and for model-checking, it's generally helpful to have quartiles of exposure:

```{r, "TRY varderive", purl=TRUE, results = "hide"}
dat_use <-
  aedat %>% arrange(USUBJID, TTE_SEVERE) %>% slice(1) %>%
  group_by(PBO) %>%
  mutate(Quartile = ifelse(PBO == "PBO", "PBO",
                           paste0("Q", ntile(CAVGSS, n = 4))))
```


## EDA: Pairs plots

```{r, "TRY covplot", purl=TRUE, results = "hide", width=10, height=10}
dat_eda <- dat_use %>% ungroup() %>%
  select(STUDYID, SEXTXT, PTTYPE, CAVGSS, BWT, Quartile) %>%
  mutate( # tell plot fn how to plot vars
   CAVGSS = as.numeric(CAVGSS),
   BWT = as.numeric(BWT)
  )

p <- ggpairs(dat_eda,
        axisLabels = "internal",
        columnLabels = c(rep("", 3), # 4 categorical variables
                         "Cavg", "Weight",  # continuous variables
                         "" # Quartile
                         )
        ) + theme_light()
p

ggsave("ggpairs.pdf", p, height=10, width=10)
```

Suppose we were planning to fit a model that included all of the following covariates:

* Sex
* Patient type
* Study
* Weight
* (as well as Cavg)

Should we expect this to be problematic?


## EDA: Categorical covariate plot

```{r}
aedat %>% 
  group_by(SEXTXT) %>% 
  summarise(x = sum(AE01), n=n()) %>% 
  mutate(phat = x / n,
         lcl = binom.confint(x,n,methods = "wilson")$lower,
         ucl = binom.confint(x,n,methods = "wilson")$upper) %>% 
  ggplot(aes(x=SEXTXT, y=phat)) + 
  geom_col() +
  geom_errorbar(aes(ymin=lcl, ymax=ucl), 
                width = 0.2) +
  labs(x='', y='Probability of a severe AE')
```


## EDA: E-R relationship

```{r, "TRY ereda", purl=TRUE, results = "hide"}
dat_eda <- dat_use %>% group_by(Quartile) %>% mutate(MedConc = median(CAVGSS))
p <- ggplot() +
  geom_rug(data = filter(dat_eda, AE01 == 0),
           aes(x = CAVGSS), sides = "b") +
  geom_rug(data = filter(dat_eda, AE01 == 1),
           aes(x = CAVGSS), sides = "t") +
  # NB: default smoother is problematic for 0-1
  # data. More on this later but live with it
  # for now.
  geom_smooth(data = dat_eda,
              aes(x = CAVGSS, y = AE01),
              method='gam', formula=y~s(x),
              method.args = list(family='binomial')) +
  stat_summary(
    data = dat_eda,
    aes(x = MedConc, y = AE01, group = MedConc),
    fun.y = function(y) sum(y) / length(y),
    geom = "point"
  ) +
  stat_summary(
    data = dat_eda,
    aes(x = MedConc, y = AE01),   
    # Wilson CI recommended by Agresti and Coull (2000) review paper
    fun.ymin = function(y) {
      binom.confint(sum(y),length(y),
                    methods = "wilson")$lower
      },
    fun.ymax = function(y) {
      binom.confint(sum(y), length(y),
                    methods = "wilson")$upper
      },
    geom = "errorbar"
  )
p
p + facet_wrap(~STUDYID) # Note exposure quartile not re-computed per-study. This is probably what you want.  
p + facet_wrap(~SEXTXT)
p + facet_wrap(~PTTYPE)
```

Exercise: list several key findings based on the EDA, and note any implications or expectations that relate to modeling these data.

```{r}

```





# Workbook: Exploring odds ratios, relative risks, and the logit transformation

## odds ratios

A function to compute the relative risk, given two conditional probabilities for the same event:

```{r, "TRY efficacy", purl=TRUE, results = "hide"}
relative_risk <- function(p0, p1) {
  return( p1 / p0 )
}
relative_risk(0.05, 0.15)
```

* Write a similar function to compute the odds ratio.
* Compare odds ratios to relative risks under a variety of scenarios

```{r}

```


## logistic function

Suppose we have the following data

```{r, "datashell", purl=TRUE, results = "hide"}
nsubj <- 160
dat <- data.frame(
  BWT = rnorm(nsubj, 75, 5),
  CAVGSS = exp(rnorm(nsubj, log(1), 1)),
  AE01 = NA # AE indicator: 1 signifies AE ; 0 signifies no AE
)
head(dat)
```

Let's plot the probability of an AE for two linear predictors
 - logit(p) = 0 + CAVGSS
 - logit(p) = -2 + 3*CAVGSS

```{r, "TRY logistic", purl=TRUE, results = "hide"}
expit <- function(x) 1 / (1+exp(-x))
ggplot(dat) +
  geom_line(aes(x = CAVGSS, y = expit(CAVGSS))) +
  geom_line(aes(x = CAVGSS, y = expit(-2 + 3 * CAVGSS)), colour = "red", linetype = 2)
```

Exercise: Modify the linear predictor as a function of `CAVGSS` to:

* Make the intercept lower
* Make the E-R relationship shallower

```{r}

```


Now, suppose we've observed these data

```{r, "TRY link", purl=TRUE, results = "hide"}
dat$Pred <- expit(-2 + 0.75 * dat$CAVGSS)
dat$AE01 <- rbinom(nrow(dat), 1, dat$Pred)
ggplot(dat) +
  geom_line(aes(x = CAVGSS, y = Pred)) +
  geom_rug(data = filter(dat, AE01 == 0), aes(x = CAVGSS), sides = "b") +
  geom_rug(data = filter(dat, AE01 == 1), aes(x = CAVGSS), sides = "t")
```


Exercise: Generate values for `AE01` so that they conform to a probit-link, rather than a logit-link model.

Hints:

* The probit function $\Phi^{-1}$ is implemented in R as `qnorm()`.
* The inverse-probit function $\Phi$ is implemented in R as `pnorm()`.

```{r}

```







# Workbook: Fitting a logistic regression model

```{r, "TRY firstglm", purl=TRUE, results = "hide"}
dat_mod <- dat_use
mod01 <- glm(AE01 ~ CAVGSS + PTTYPE,
             family = binomial(link = "logit"),
             data = dat_mod,
             )
```

Exercise:
* Re-fit the model using a probit link (mod01_probit)
* Re-fit the model with STUDYID as an additional covariate (mod02)

```{r}

```


## reading R help

R help pages can be overwhelming.  Nonetheless, take a look at the following help files.

```{r, "TRY extractors", purl=TRUE, results = "hide"}
?glm # See especially the "See Also" section
?summary.glm # See expecially the "Value" section
?confint
?predict
?predict.glm
```

Exercise:

* Print out a table of estimated coefficients and their standard errors.
* Print out 95% confidence intervals for the model parameters.
* Print out 90% confidence intervals for the model parameters.

```{r}

```


## model intercept

```{r, "TRY intercept", purl=TRUE, results = "hide"}
partab01 <- cbind(coef(mod01), confint(mod01))
partab01
```

Exercise:

* Make the intercept more interpretable by applying the inverse of the link function.
* What exposure and covariate settings does the intercept correspond to?

```{r}

```


## re-coding predictors

```{r, "TRY recoding", purl=TRUE, results = "hide"}
ref_bwt <- median(dat_use$BWT) # will want reference value later, so save as variable!
dat_mod <- dat_use %>% mutate(
  BWT_norm = BWT - ref_bwt,
  PTTYPE = factor(PTTYPE, levels = c("PT1", "PT2", "HV"))
  )
?relevel
```

Exercise: 
* Re-fit model `mod01` adding the effects of PTTYPE and normalize body weight.  What exposure and covariate settings does the intercept corresponds to?

```{r}

```

* How many fold higher are the odds of an AE for `PT1` versus `HV`?

```{r}

```






# Workbook: Model evaluation and comparison


## residual plot


```{r, "TRY resid", purl=TRUE, results = "hide"}
dat_plus <- dat_mod
dat_plus$res <- residuals(mod01, type = "response")
dat_plus$pred <- fitted(mod01)

resplot <-
  ggplot(dat_plus, aes(x = CAVGSS, y = res)) +
  geom_point() +
  geom_smooth()

resplot  
resplot + facet_wrap(~ PTTYPE)

# deviance residuals
dat_plus$resd <- residuals(mod01, type = "deviance")
dat_plus$pred <- fitted(mod01)

resplot <-
  ggplot(dat_plus, aes(x = CAVGSS, y = resd)) +
  geom_point() +
  geom_smooth()

resplot  
resplot + facet_wrap(~ PTTYPE)

```

Suggestive of:

* Over-predicting rates for HV at high exposures.
* Possibly underpredicting at mid-range exposures for `PT1`.

Exercise: Plot residuals by `BWT`, facetting by `STUDYID`.

```{r}

```


Exercise:

Binned residuals are another way to smooth out the discreteness in binary residuals.
The general algoirthm is to:

1. Bin the data by fitted value
2. Within each bin, calculate the average
    * residual
    * fitted probablity of the outcome
    * continuous covariate values
3. Plot the average residual (y axis) against the other average metrics per bin
4. Include thresholds for the residuals at +/- 2*sqrt(p_bin(1-p_bin)/n_bin), where p_bin is
    is the fitted probability

```{r}

```

## pred v. conc

```{r, "TRY predconc", purl=TRUE, results = "hide"}
p <- ggplot(dat_plus, aes(x = CAVGSS)) +
  geom_rug(data = filter(dat_plus, AE01 == 0),
           sides = "b") +
  geom_rug(data = filter(dat_plus, AE01 == 1),
           sides = "t") +
  # NB: default smoother is problematic for 0-1
  # data. More on this later but live with it
  # for now.
  geom_smooth(aes(y = AE01))

p
p + geom_line(aes(y = pred))
p + geom_line(aes(y = pred)) + facet_wrap(~PTTYPE)
```

Exercise: make a version of the previous plot that smooths out the predicted values.

```{r}

```

# DHARMa residuals


```{r}

simulationOutput = DHARMa::simulateResiduals(mod01, n = 1000)
plot(simulationOutput)
DHARMa::plotResiduals(simulationOutput)
DHARMa::plotResiduals(simulationOutput,form=dat_mod$CAVGSS,xlab="CAVE")

```

## VPC vs a categorical covariate

```{r, "TRY glmvpc", purl=TRUE, results = "hide"}
prop <- function(x) sum(x, na.rm = TRUE) / sum(!is.na(x))
obs_stat <-
  dat_mod %>%
  group_by(PTTYPE, Quartile) %>%
  summarise(pAE = prop(AE01))
ggplot(obs_stat) +
  geom_point(aes(x = Quartile, y = pAE)) +
  facet_wrap(~ PTTYPE)
```

```{r, "TRY2 glmvpc", purl=TRUE, results = "hide"}
#' @title  Simulate nSim trials from a glm object
#' @description Given a glm model, simulate trials without parameter uncertainty
#' @param mod A glm model object
#' @param nsim Number of simulation replicates (integer)
#' @return nobs x nsim matrix of simulated values
simulate.glm <- function(mod, nsim) {  
    phat <- predict(mod, type = "response")
    simy <- replicate(nsim, rbinom(length(phat), 1, phat))
    simy
}
sim_dat <- simulate(mod03, 100) # running this "quick and dirty", but could easily bump reps to 1000

# Function to calculate predicted prob of event per PTTYPE and quartile
stat_fn <- function(simy, dat_orig = dat_mod) {
  dati <- dat_orig
  dati$AE01 <- simy
  dati %>%
    group_by(PTTYPE, Quartile) %>%
    summarise(pAE = prop(AE01))
}

sim_stat <- apply(sim_dat, 2, stat_fn) %>% bind_rows()

sim_stat_q <-
  sim_stat %>%
  group_by(PTTYPE,Quartile) %>%
  summarize(LB=quantile(pAE, prob=c(0.05)),
            MED=median(pAE),
            UB=quantile(pAE, prob=.95)
  )

comb_stat <- full_join(obs_stat, sim_stat_q)

ggplot(data = comb_stat) +
  geom_pointrange(aes(x = Quartile, y = MED, ymin = LB, ymax = UB, colour = "Predicted")) +
  geom_point(aes(x = Quartile, y = pAE, colour = "Observed"), size = 3, alpha = 0.5) +
  facet_wrap(~ PTTYPE) + theme_bw()
```

Exercise: Create a VPC for the proportion of patients with an AE within each gender.

```{r}

```

## VPC against continuous predictors

```{r, "TRY continvpc", purl=TRUE, results = "hide"}
obs_stat <-
  dat_mod %>%
  ungroup %>%
  mutate(pAE=predict(loess(AE01 ~ CAVGSS, data = dat_mod))) %>%
  group_by(CAVGSS) %>%
  summarize(pAE=mean(pAE)) # Could use distinct, should be identical

stat_fn <- function(simy, dat_orig = dat_mod) {
  dati <- dat_orig
  dati$AE01 <- simy
  dati$pAE <- predict(loess(AE01 ~ CAVGSS, data = dati))
  dati %>% group_by(CAVGSS) %>% summarize(pAE=mean(pAE))
}

sim_stat <-
  data.frame(sim_dat) %>%
  select_all() %>%
  map(~stat_fn(.)) %>%
  bind_rows

sim_stat_q <-
  sim_stat %>%
  group_by(CAVGSS) %>%
  summarize(LB=quantile(pAE, prob=c(0.05)),
            MED=median(pAE),
            UB=quantile(pAE, prob=.95)
  )

comb_stat <- full_join(obs_stat, sim_stat_q)

ggplot(data = comb_stat) +
  geom_ribbon(aes(x = CAVGSS, ymin = LB, ymax = UB, colour = "Predicted"), fill = "lightgrey") +
  geom_line(aes(x = CAVGSS, y = MED, colour = "Predicted"), size = 3) +
  geom_line(aes(x = CAVGSS, y = pAE, colour = "Observed"), size = 3) +
  theme_bw()
```

Exercise: make a similar VPC plotted against bodyweight.

```{r}

```


Exercise: Our model includes an interacation term, make a VPC of CAVGSS by PTTYPE

```{r}

```


Extended Exercise:

* Re-fit the first model (mod01) to allow a different slope of exposure-response relationship for each level of `PTTYPE`. 
(Hint: the formula will be AE01 ~ CAVGSS*PTTYPE)
* Re-generate the residual and pred v. conc plots and compare the results.
* Read `?aic` and compare the models with and without interaction in terms of AIC.

```{r}

```


```{r, "TRY lincombo", purl=TRUE, results = "hide"}
lin_combo <- function(fit, k.lin) {
  est <- coef(fit) %*% k.lin  
  V <- t(k.lin) %*% vcov(fit) %*% k.lin
  se <- sqrt(V)
  out <- c(est, se)
  out
}
names(coef(mod03))
which.pars <- names(coef(mod03)) %in% c("CAVGSS", "CAVGSS:PTTYPEHV")
as.numeric(which.pars)
lin_combo(mod03, as.numeric(which.pars))
```

Compare the above result with what you get when you re-fit the model with `HV` as the reference level.

```{r}

```



## Conditional predictions

```{r, "TRY forest", purl=TRUE, results = "hide"}
# We need some reference exposure levels:
ref_exp <- 1:3
# and some reference bodyweights:
ref_bwt_norm <- c(65, 75, 85) - ref_bwt
# For categorical covariates it is more straightforward:
ref_sex <- unique(dat_mod$SEXTXT)
ref_type <- unique(dat_mod$PTTYPE)
# Now let's make a grid of setting where we want predictions:
pred_frame <- expand.grid(BWT_norm = ref_bwt_norm,
                          CAVGSS = ref_exp,
                          SEXTXT = ref_sex,
                          PTTYPE = ref_type
                          )

pred_frame$Pred <- predict(mod03, newdata = pred_frame)
pred_frame$PredSE <- predict(mod03, newdata = pred_frame, se.fit = TRUE)$se.fit
pred_frame <- pred_frame %>%
  mutate(
    LB = 100 * expit(Pred + qnorm(0.025) * PredSE),
    EST = 100 * expit(Pred),
    UB = 100 * expit(Pred + qnorm(0.975) * PredSE)
  )
pred_frame$BWT <- round(pred_frame$BWT_norm + ref_bwt, 0)
pred_frame$Label = with(pred_frame, paste(PTTYPE, SEXTXT, BWT, CAVGSS))

pred_frame

ggplot(data = filter(pred_frame, PTTYPE != "HV")) +
  geom_pointrange(aes(x=Label, y=EST, ymin=LB, ymax=UB)) +
  coord_flip() + facet_wrap(~PTTYPE, scales="free_y") +
  xlab("Estimated AE Rate")

ggplot(data = filter(pred_frame, PTTYPE != "HV")) +
  geom_pointrange(aes(x=Label, y=EST, ymin=LB, ymax=UB)) +
  coord_flip() + facet_wrap(~SEXTXT, scales="free_y") +
  xlab("Estimated AE Rate")

```

Exercise: Re-fit the model without any covariate-expsoure interactions and re-make the "forest plot" of conditional predictions.

```{r}

```


## Marginal predictions

When making marginal predictions, we wish to describe an average (marginal) effect over some population.

Let's compare marginal predictions of CAVGSS for PT1 and PT2 like those studied:

```{r, "TRY marginal-cavg-pttype", purl=TRUE}
pt1pats <- filter(dat_mod, PTTYPE=="PT1") %>%
  sample_n(10000,replace=TRUE) %>%
  select(CAVGSS,PTTYPE,BWT_norm,SEXTXT)

pt2pats <- filter(dat_mod, PTTYPE=="PT2") %>%
  sample_n(10000,replace=TRUE) %>%
  select(CAVGSS,PTTYPE,BWT_norm,SEXTXT)

pt1pats %>% ungroup %>% summary
pt2pats %>% ungroup %>% summary

CAVGSS_at <- seq(min(dat_mod$CAVGSS[dat_mod$CAVGSS>0]),
                 max(dat_mod$CAVGSS), length.out = 50)

pats <- bind_rows(pt1pats,pt2pats)%>% ungroup
pats$CAVGSS <- NULL

pred_eff <-
  map_df(seq_along(CAVGSS_at), function(CAVGSS_i){
    pats %>%
      mutate(CAVGSS=CAVGSS_at[CAVGSS_i]) %>%
      mutate(
        xb=predict(mod03, newdata=., type="link")
      ) %>%
      group_by(CAVGSS,PTTYPE) %>%
      summarize(pAE=expit(mean(xb)),
                ul=expit(mean(xb)+2*sd(xb)),
                ll=expit(mean(xb)-2*sd(xb)))
  })

pred_eff %>%
  ggplot(aes(x=CAVGSS, y=pAE, color=PTTYPE, fill=PTTYPE, ymax=ul, ymin=ll)) +
  geom_line() + theme_bw() +
  geom_ribbon(alpha=.25) +
  ggtitle("Marginal effect")
```

Exercise:
Suppose that exposure varies with bodyweight, plot the net effect across studied weights.

```{r}

```


## Clinical trial simulation

There is a planned Ph2 study in 100 PT2 patients.  
The patients will be dosed with a target CAVGSS of 2.5, and we expect values to fall
between 2.25 and 2.75 (say it varies normally around 2.5).  

* What rate of serious AE's should we expect in this trial?
* Compare with and without parameter uncertainty

Hint:
? MASS::mvrnorm
? model.matrix
? model.frame

```{r}
```


## Get your GAM on

```{r}
?s
mod04 <- gam(AE01 ~ s(CAVGSS,by=PTTYPE) + PTTYPE + BWT_norm + SEXTXT,
             data=dat_mod, family = "binomial")
```

```{r, "TRY marginal-cavg-pttype-gam", purl=TRUE}
pt1pats <- filter(dat_mod, PTTYPE=="PT1") %>%
  ungroup() %>%
  sample_n(10000,replace=TRUE) %>%
  select(CAVGSS,PTTYPE,BWT_norm,SEXTXT)

pt2pats <- filter(dat_mod, PTTYPE=="PT2") %>%
  ungroup() %>%
  sample_n(10000,replace=TRUE) %>%
  select(CAVGSS,PTTYPE,BWT_norm,SEXTXT)

pt1pats %>% ungroup %>% summary
pt2pats %>% ungroup %>% summary

CAVGSS_at <- seq(min(dat_mod$CAVGSS[dat_mod$CAVGSS>0]),
                 max(dat_mod$CAVGSS), length.out = 50)

pats <- bind_rows(pt1pats,pt2pats)%>% ungroup
pats$CAVGSS <- NULL

pred_eff <-
  map_df(seq_along(CAVGSS_at), function(CAVGSS_i){
    pats %>%
      mutate(CAVGSS=CAVGSS_at[CAVGSS_i])  %>%
      mutate( xb=predict(mod04, newdata=., type="link") ) %>%
      group_by(CAVGSS,PTTYPE) %>%
      summarize(mxb=mean(xb), sxb=sd(xb)) %>%
      mutate(pAE = expit(mxb), ul=expit(mxb+2*sxb), ll=expit(mxb-2*sxb))
  })

pred_eff %>%
  ggplot(aes(x=CAVGSS, y=pAE, color=PTTYPE, fill=PTTYPE,ymax=ul, ymin=ll)) +
  geom_line(lwd=2) + theme_bw() +
  geom_ribbon(alpha=.25) +
  ggtitle("Marginal effect")
```