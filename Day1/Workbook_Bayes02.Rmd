---
title: "Workbook Bayes02"
author: "Model evaluation and comparison"
date: "`r Sys.Date()`"
output:
  html_document:
#    css: docs/src/styles/styles.css
    number_sections: true
    theme: united
    toc: true
    toc_float: true
params:
  include: TRUE
---


```{r,echo=FALSE,message=FALSE}
knitr::opts_chunk$set(comment='.',fig.align=TRUE,message=FALSE,warning=FALSE)
theme_set(theme_bw())
```


# Preliminaries for R examples

```{r, message=FALSE}
library(tidyverse)
library(stringr)
library(haven)
library(survival)
library(GGally)
library(binom)
library(texreg)
library(mgcv)
library(brms)
library(tidybayes)
library(DHARMa)
library(modelr)

expit <- function(x) 1 / (1+exp(-x))
# note plogs = expit
```

# Slides01
# Slides02
## Toy data

```{r, "TRY toydata", purl=TRUE, results = "hide"}
load('../data/aedat.RDS')
source('preprocess_data.R')
```


## Re-fit model 01

Fit the model with main effects for CAVGSS and PTTYPE using normal(0,5) prior distributions.

```{r, "TRY firststan", purl=TRUE, results = "hide"}
mod01 <- brm(formula = AE01 ~ CAVGSS + PTTYPE,  
             data = aedat, 
             family = bernoulli(link = "logit"),
             warmup = 500, 
             iter = 2000, 
             chains = 4, 
             inits = "random", 
             cores = 4,
             prior = set_prior('normal(0,5)',class='b'),
             seed = 123)
```


For comparison, also fit a model that allows the exposure-response slope to differ by patient type.

```{r, "TRY firststan", purl=TRUE, results = "hide"}
mod01_ix <- brm(formula = AE01 ~ CAVGSS * PTTYPE,  
                data = aedat, 
                family = bernoulli(link = "logit"),
                warmup = 500, 
                iter = 2000, 
                chains = 4, 
                inits = "random", 
                cores = 4,
                prior = set_prior('normal(0,5)',class='b'),
                seed = 123)
```

**Exercise:**
* Check convergence diagnostics for mod01_ix
* Look at the posterior summaries for the parameters (try using both `mcmc_plot(mod01_ix, type='intervals')` and `summary(mod01_ix)`)
* What would you conclude about the effect of exposure?  Does your inference based on this model differ from mod01?


# Compare models using WAIC and LOO-CV

We'll use the `add_criteria` function to save the WAIC and loo output as part of the model object:

```{r}
mod01 <- add_criterion(mod01, c("loo","waic"))
```

To look at this we can print the list
```{r}
print(mod01$criteria$loo)
print(mod01$criteria$waic)
```

This is the same as the output from the `loo` function:

```{r}
loo(mod01)
waic(mod01)
```


**Exercise:**
* Add WAIC and LOO-CV to mod01_ix
* Compare the WAIC and LOO-CV values using the `loo_compare` function
* Which model is preferred using these criteria?  Is it the same model you would select based on looking at the posterior distributions?


# Residuals


To use the quantile residuals, we'll need a sample from the posterior predictive distribution.  We can get this with the `posterior_predict` function.

```{r}
postpred_sample_mod01 = posterior_predict(mod01)
dim(postpred_sample_mod01)
```

This yields a matrix with one row per posterior sample and one column per observation (subject).  Let's look at the first 5 samples for the first 10 subjects:

```{r}
postpred_sample_mod01[1:5,1:10]
```


Now we can use the `DHARMa::createDHARMa` function to calculate the quantile residuals

```{r}
dharma_resids = createDHARMa(simulatedResponse = t(postpred_sample_mod01),
                             observedResponse = aedat$AE01,
                             integerResponse = TRUE)
```

```{r}
plot(dharma_resids)
```

Let's look at the residuals vs exposure. Ideally, the residuals should have a mean of 0.5 for all values of exposure.  How does this look to you?

```{r}
aedat <- aedat %>% ungroup() %>% 
  mutate(quantile_residual = dharma_resids$scaledResiduals)
```

```{r}
aedat %>% 
  ggplot(aes(x=CAVGSS, y=quantile_residual)) +
  geom_point() +
  geom_smooth()
```


**Exercise:**
* Repeat the above residual plots for mod01_ix.  Do they look substantively better than for mod01?


## Posterior predictive checks

We'll focus our model evaluation on the exposure-response relationship and use the same approach to our summary statistic as in the notes:
* Fit generalized additive model (smoother) to each simulated dataset
* Predict at a fixed grid of values (0 to 95$^{th}$ percentile)

First,, we'll simulate data from both of our models

```{r}
aedat_pp01 = add_predicted_draws(model=mod01, newdata = aedat)
aedat_pp01_ix = add_predicted_draws(model=mod01_ix, newdata = aedat)
```

Next, we'll define the summary function.  Take some time to understand what this function is doing.

```{r}
summary_function <- function(.data, .x_name, .y_name='value') {
  # Give x and y variables generic names
  .data <- .data %>% 
    ungroup() %>% 
    rename('xvar' = all_of(.x_name), 
           'yvar' = all_of(.y_name))
  # Determine grid of values at which to evaluate function
  x_grid <- with(.data, seq(from = min(xvar),
                           to = quantile(xvar,probs = 0.95), 
                           length = 100))
  # Fit GAM 
  fit <- gam(yvar ~ s(xvar), family=binomial(link='logit'), data=.data)
  # Obtain predictions
  predictions <- predict(fit, newdata = data.frame(xvar=x_grid), type='response')
  return( data.frame(xvar=x_grid, prediction = predictions))
}
```


Next, we'll apply the summary function separately to groups stratified by patient type:

```{r}
obs_summary <- aedat %>% 
  nest(cols=-PTTYPE) %>% 
  mutate(preds = map(cols, function(.x) {
    summary_function(.x, .x_name = 'CAVGSS', .y_name='AE01')})
    ) %>% 
  select(-cols) %>% 
  unnest(cols = preds) %>% 
  mutate(type='Observed')
```

We'll start with the vpc for mod01_ix by computing the summary on each simulated study.
You might get some warning messages regarding the iteration limit being reached before convergence.  For the sake of time, we'll ignore them, but what do you think is likely to be causing them?

```{r}
sim_summary_01ix <- aedat_pp01_ix %>%
  # Use 200 sims for demonstration
  semi_join(data.frame(.draw = sample(1:max(aedat_pp01_ix$.draw), size=500))) %>% 
  # Nest everying except the simulation name
  group_by(.draw,PTTYPE) %>% 
  nest() %>% 
  # Compute summary stats for each simulated dataset
  mutate(predictions = map(data, ~summary_function(.x, 
                                                   .x_name='CAVGSS',
                                                   .y_name='.prediction'))) %>% 
  select(.draw,PTTYPE, predictions) %>% 
  unnest(cols=predictions) %>% 
  # Summarise across simulated data sets
  group_by(xvar,PTTYPE) %>% 
  summarise(qlo = quantile(prediction, probs = 0.05),
            qhi = quantile(prediction, probs = 0.95),
            prediction=median(prediction)
            ) %>% 
  mutate(type = 'Simulated')
```

# Plot VPC

```{r, 'cont-vpc'}
sim_summary_01ix %>% 
  bind_rows(obs_summary) %>% 
  ggplot(aes(x=xvar, y=prediction)) +
  geom_line(aes(col=type, group=type)) +
  geom_ribbon(aes(ymin=qlo, ymax=qhi, fill=type), alpha=0.2) +
  facet_wrap(~PTTYPE) +
  labs(x='Steady-state Cavg', y='Probability of severe AE')

```


**Exercise:**
* Repeat the above VPCs for mod01.  Do they look substantively worse than for mod01_ix?

